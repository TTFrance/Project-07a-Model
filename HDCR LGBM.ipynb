{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first attempt at getting a working LGBM Model running\n",
    "\n",
    "It works, but the scores are terrible\n",
    "\n",
    "The best model is pickled to 'model/best_LGBM.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 798)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_SAVED_DATA = True\n",
    "\n",
    "if USE_SAVED_DATA:\n",
    "    print ('Using saved data')\n",
    "    df = pd.read_csv('testdata/hdcr10k.csv')\n",
    "else:\n",
    "    print ('Using FULL data')\n",
    "    df = pd.read_csv('../data/cleaned/hcdr_FULL.csv',nrows=10000)\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    df.to_csv('testdata/hdcr10k.csv', index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    9225\n",
       "1.0     775\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058717\n"
     ]
    }
   ],
   "source": [
    "print (df.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPUTE NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy = 'median')\n",
    "df=pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "df=pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "    'EXT_SOURCE_3',\n",
    "    'EXT_SOURCE_2',\n",
    "    'PAYMENT_RATE',\n",
    "    'DAYS_ID_PUBLISH',\n",
    "    'EXT_SOURCE_1',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED',\n",
    "    'DAYS_EMPLOYED_PERC',\n",
    "    'REGION_POPULATION_RELATIVE',\n",
    "    'DAYS_REGISTRATION',\n",
    "    'ANNUITY_INCOME_PERC',\n",
    "    'DAYS_LAST_PHONE_CHANGE',\n",
    "    'AMT_GOODS_PRICE',\n",
    "    'AMT_ANNUITY',\n",
    "    'INCOME_CREDIT_PERC',\n",
    "    'AMT_CREDIT',\n",
    "    'INCOME_PER_PERSON',\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'HOUR_APPR_PROCESS_START',\n",
    "    'TARGET']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print (df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 19)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X= df.drop('TARGET',axis=1)\n",
    "y= df['TARGET']\n",
    "\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT TEST/TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.761 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.712 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.761 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.712 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.727 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.759 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.708 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.731 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.759 total time=   0.1s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.708 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.731 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.756 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.744 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.763 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.714 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.756 total time=   0.1s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.744 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.763 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.714 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.756 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.744 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.764 total time=   0.1s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.715 total time=   0.0s\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.756 total time=   0.0s\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.744 total time=   0.0s\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.721 total time=   0.0s\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.764 total time=   0.0s\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.715 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=1001, shuffle=True),\n",
       "             estimator=LGBMClassifier(random_state=0),\n",
       "             param_grid={'colsample_bytree': [0.9497036],\n",
       "                         'learning_rate': [0.1, 0.02], 'max_depth': [5, 8],\n",
       "                         'min_child_weight': [39.3259775],\n",
       "                         'min_split_gain': [0.0222415], 'n_estimators': [100],\n",
       "                         'num_leaves': [34, 40], 'reg_alpha': [0.041545473],\n",
       "                         'reg_lambda': [0.0735294], 'subsample': [0.8715623]},\n",
       "             scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "folds = KFold(n_splits= 5, shuffle=True, random_state=1001)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate':[0.1,0.02],\n",
    "    'num_leaves':[34,40],\n",
    "    'colsample_bytree':[0.9497036],\n",
    "    'subsample':[0.8715623],\n",
    "    'max_depth':[5, 8],\n",
    "    'reg_alpha':[0.041545473],\n",
    "    'reg_lambda':[0.0735294],\n",
    "    'min_split_gain':[0.0222415],\n",
    "    'min_child_weight':[39.3259775],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LGBMClassifier(random_state=0), params, scoring='roc_auc', cv=folds, verbose=3)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7437966330993124\n",
      "LGBMClassifier(colsample_bytree=0.9497036, max_depth=5,\n",
      "               min_child_weight=39.3259775, min_split_gain=0.0222415,\n",
      "               num_leaves=34, random_state=0, reg_alpha=0.041545473,\n",
      "               reg_lambda=0.0735294, subsample=0.8715623)\n",
      "{'colsample_bytree': 0.9497036, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 39.3259775, 'min_split_gain': 0.0222415, 'n_estimators': 100, 'num_leaves': 34, 'reg_alpha': 0.041545473, 'reg_lambda': 0.0735294, 'subsample': 0.8715623}\n"
     ]
    }
   ],
   "source": [
    "print (grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.5080261136712749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_predict = best_model.predict(X_test)\n",
    "print (\"ROC AUC score:\",roc_auc_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/best_LGBM.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, 'model/best_LGBM.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[10:58:18] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.751 total time=   0.4s\n",
      "[10:58:18] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.754 total time=   0.3s\n",
      "[10:58:19] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.716 total time=   0.3s\n",
      "[10:58:19] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.777 total time=   0.3s\n",
      "[10:58:20] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.727 total time=   0.3s\n",
      "[10:58:20] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.751 total time=   0.3s\n",
      "[10:58:20] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.754 total time=   0.3s\n",
      "[10:58:21] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.716 total time=   0.3s\n",
      "[10:58:21] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.777 total time=   0.3s\n",
      "[10:58:22] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.727 total time=   0.3s\n",
      "[10:58:22] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.4s\n",
      "[10:58:22] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.755 total time=   0.4s\n",
      "[10:58:23] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.711 total time=   0.4s\n",
      "[10:58:23] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.775 total time=   0.4s\n",
      "[10:58:24] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.726 total time=   0.4s\n",
      "[10:58:24] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.749 total time=   0.4s\n",
      "[10:58:25] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.755 total time=   0.4s\n",
      "[10:58:25] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.711 total time=   0.4s\n",
      "[10:58:26] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.775 total time=   0.4s\n",
      "[10:58:26] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.1, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.726 total time=   0.4s\n",
      "[10:58:27] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.750 total time=   0.4s\n",
      "[10:58:27] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.732 total time=   0.4s\n",
      "[10:58:28] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.706 total time=   0.5s\n",
      "[10:58:29] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.775 total time=   0.4s\n",
      "[10:58:29] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.719 total time=   0.4s\n",
      "[10:58:30] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.750 total time=   0.4s\n",
      "[10:58:30] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.732 total time=   0.4s\n",
      "[10:58:31] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.706 total time=   0.4s\n",
      "[10:58:31] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.775 total time=   0.4s\n",
      "[10:58:32] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=5, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.719 total time=   0.4s\n",
      "[10:58:32] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.752 total time=   0.5s\n",
      "[10:58:33] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.737 total time=   0.5s\n",
      "[10:58:33] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.708 total time=   0.5s\n",
      "[10:58:34] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.777 total time=   0.5s\n",
      "[10:58:35] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=34, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.719 total time=   0.5s\n",
      "[10:58:35] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 1/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.752 total time=   0.5s\n",
      "[10:58:36] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.737 total time=   0.6s\n",
      "[10:58:37] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 3/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.708 total time=   0.5s\n",
      "[10:58:37] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 4/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.777 total time=   0.6s\n",
      "[10:58:38] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.9497036, learning_rate=0.02, max_depth=8, min_child_weight=39.3259775, min_split_gain=0.0222415, n_estimators=100, num_leaves=40, reg_alpha=0.041545473, reg_lambda=0.0735294, subsample=0.8715623;, score=0.719 total time=   0.5s\n",
      "[10:58:39] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=1001, shuffle=True),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constrai...\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             param_grid={'colsample_bytree': [0.9497036],\n",
       "                         'learning_rate': [0.1, 0.02], 'max_depth': [5, 8],\n",
       "                         'min_child_weight': [39.3259775],\n",
       "                         'min_split_gain': [0.0222415], 'n_estimators': [100],\n",
       "                         'num_leaves': [34, 40], 'reg_alpha': [0.041545473],\n",
       "                         'reg_lambda': [0.0735294], 'subsample': [0.8715623]},\n",
       "             scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "folds = KFold(n_splits= 5, shuffle=True, random_state=1001)\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(xgb_model, params, scoring='roc_auc', cv=folds, verbose=3, refit=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448052403819625\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1,\n",
      "              colsample_bytree=0.9497036, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
      "              max_cat_to_onehot=4, max_delta_step=0, max_depth=5, max_leaves=0,\n",
      "              min_child_weight=39.3259775, min_split_gain=0.0222415,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_leaves=34, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=0, ...)\n",
      "{'colsample_bytree': 0.9497036, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 39.3259775, 'min_split_gain': 0.0222415, 'n_estimators': 100, 'num_leaves': 34, 'reg_alpha': 0.041545473, 'reg_lambda': 0.0735294, 'subsample': 0.8715623}\n"
     ]
    }
   ],
   "source": [
    "print (grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:58:51] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"min_split_gain\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "ROC AUC score: 0.5080261136712749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_predict = best_model.predict(X_test)\n",
    "print (\"ROC AUC score:\",roc_auc_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/best_XGBOOST.pkl']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, 'model/best_XGBOOST.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('OCP7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a021ee55297eef2d09131013fb40c2be9107a7aec06b872979238f8fe03dbee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
